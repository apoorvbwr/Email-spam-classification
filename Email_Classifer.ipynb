{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bNS_VvNXzeKd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JBgNJWwY0kVW"
   },
   "outputs": [],
   "source": [
    "download_root = \"http://spamassassin.apache.org/old/publiccorpus/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xElT71zZ1Pox"
   },
   "outputs": [],
   "source": [
    "ham_url = download_root + \"20030228_easy_ham.tar.bz2\"\n",
    "spam_url = download_root + \"20030228_spam.tar.bz2\"\n",
    "spam_path = os.path.join('datasets','spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rtqXqoB21dM_"
   },
   "outputs": [],
   "source": [
    "def fetch_spam_data(spam_url = spam_url,spam_path = spam_path):\n",
    "  if not os.path.isdir(spam_path):\n",
    "    os.makedirs(spam_path)\n",
    "  for filename, url in (('ham.tar.bz2',ham_url),('spam.tar.bz2',spam_url)):\n",
    "    path = os.path.join(spam_path,filename)\n",
    "    if not os.path.isfile(path):\n",
    "      urllib.request.urlretrieve(url,path)\n",
    "    tar_bz2_file = tarfile.open(path)\n",
    "    tar_bz2_file.extractall(path = spam_path)\n",
    "    tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Sz5DR7tc2x9c"
   },
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8RYZ5pvw25rD"
   },
   "outputs": [],
   "source": [
    "# Let's load all the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WrhDRtfm3VpH"
   },
   "outputs": [],
   "source": [
    "ham_dir = os.path.join(spam_path,'easy_ham')\n",
    "spam_dir = os.path.join(spam_path,'spam')\n",
    "ham_filesname = [name for name in sorted(os.listdir(ham_dir)) if len(name)>20]\n",
    "spam_filesname = [name for name in sorted(os.listdir(spam_dir)) if len(name)>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zwPF_nc42JP",
    "outputId": "5427294d-9c3f-47a4-a460-f5255efcadc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_filesname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlhjnTZF5l3Z",
    "outputId": "c5aac3bc-1ab0-4ec0-a08d-5b63628b772e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filesname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-XHbsU65z2t"
   },
   "source": [
    "We can use Python's email module to parse these emails (this handles headers, encoding, and so on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8mhhKLOR5rwf"
   },
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "def load_email(is_spam,filename,spam_path=spam_path):\n",
    "  directory = 'spam' if is_spam else 'easy_ham'\n",
    "  with open(os.path.join(spam_path,directory,filename),\"rb\") as f:\n",
    "    return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "AhPaIqMd7FV2"
   },
   "outputs": [],
   "source": [
    "ham_emails = [load_email(is_spam=False,filename=name) for name in ham_filesname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "j8FUsKFg_QHe"
   },
   "outputs": [],
   "source": [
    "spam_emails = [load_email(is_spam=True,filename=name) for name in spam_filesname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UyxkTs0AKLM",
    "outputId": "4131642d-e841-4682-8e3f-07b4a45b650c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">                                 You have multiple generations of\n",
      "> peasants/squatters that cultivate and live on the lands almost as a\n",
      "> human parts of the property package.\n",
      "\n",
      "When I'd read that \"getting legal title\n",
      "can take 20 years\", when I believe that\n",
      "1 year ought to be more than sufficient,\n",
      "(and helped by the Cairo reference) I'd\n",
      "assumed that we were talking about the\n",
      "urban poor.\n",
      "\n",
      "If I see people living in mansions, or\n",
      "even in suburban subdivisions, I assume\n",
      "they didn't have too much trouble with\n",
      "their titles.\n",
      "\n",
      "If I see people living in shanties and\n",
      "haphazard alleyways, I tend to assume\n",
      "their parcels weren't exactly recorded\n",
      "on the government maps, or paid for with\n",
      "a bank loan, especially when nearby vacant\n",
      "lots have shotgun wielding men presumably\n",
      "intent on keeping them \"development\" free.\n",
      "\n",
      "Now, it may be that \"Manhattanites' view\n",
      "of America\" to say that outside of Metro\n",
      "Manila, Davao, and maybe another city or\n",
      "two (Cebu?), everything else (literally)\n",
      "is the boondocks.  But going on that very\n",
      "broad assumption, I guess I'm describing\n",
      "the flip side of Mr. Roger's experience:\n",
      "the paisanos (who leave behind those who\n",
      "remain on a patron's rural land) move to\n",
      "Manila, and (the second assumption) squat\n",
      "in shantytowns there, at least until they\n",
      "can line up a middle-class job.\n",
      "\n",
      "So, going on two large assumptions, I can\n",
      "come up with a scenario under which title\n",
      "would take 20 years: a shantytown arises\n",
      "somewhere in the midst of a section (or\n",
      "whatever the Spanish used to divvy up the\n",
      "land) and it takes decades of arguing to\n",
      "put together a package which somehow can\n",
      "both compensate the owner and record lots\n",
      "for the inhabitants.  Just transferring\n",
      "title to an existing lot, between parties\n",
      "who have money, ought not to be a problem.\n",
      "\n",
      "The obvious solution, at least to us\n",
      "barking farting chihuahuas on FoRK, is\n",
      "to \"introduce market mechanisms\".  It is\n",
      "left as an exercise to come up with one\n",
      "which works when many of the agents (are\n",
      "perceived to) have negligible NPV.\n",
      "\n",
      "-Dave\n",
      "\n",
      ">              [land reform] meant that all the agricultural producers had\n",
      "> to plant crops all the time (profitable or not) ...\n",
      "\n",
      "What happened to more highly-capitalized\n",
      "land?  Putting in trees instead of crops\n",
      "sounds like it might sidestep that.\n",
      "\n",
      "> Mr. Long, I think you'd particularly enjoy the De Soto work.\n",
      "\n",
      "On the \"to find\" list.  Any chance of\n",
      "an explanation of that \"Bell Jar\" in\n",
      "the meantime?\n",
      "\n",
      "http://xent.com/mailman/listinfo/fork\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[25].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJaGEo52AghM",
    "outputId": "7e7df610-40a6-481b-dafa-c65417e22fec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Fight The Risk of Cancer!\n",
      "http://www.adclick.ws/p.cfm?o=315&s=pk007\n",
      "\n",
      "2) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\n",
      "http://www.adclick.ws/p.cfm?o=249&s=pk007\n",
      "\n",
      "3) Get the Child Support You Deserve - Free Legal Advice\n",
      "http://www.adclick.ws/p.cfm?o=245&s=pk002\n",
      "\n",
      "4) Join the Web's Fastest Growing Singles Community\n",
      "http://www.adclick.ws/p.cfm?o=259&s=pk007\n",
      "\n",
      "5) Start Your Private Photo Album Online!\n",
      "http://www.adclick.ws/p.cfm?o=283&s=pk007\n",
      "\n",
      "Have a Wonderful Day,\n",
      "Offer Manager\n",
      "PrizeMama\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you wish to leave this list please use the link below.\n",
      "http://www.qves.com/trim/?ilug@linux.ie%7C17%7C114258\n",
      "\n",
      "\n",
      "-- \n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0jDhbAr2A-M7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1XqOJaHBTYB"
   },
   "source": [
    "Some emails are actually multipart, with images and attachments (which can have their own attachments). Let's look at the various types of structures we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "HqsZAGKIBUhm"
   },
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "  if isinstance(email,str):\n",
    "    return email\n",
    "  payload = email.get_payload()\n",
    "  if isinstance(payload,list):\n",
    "    return \"multipart({})\".format(','.join([get_email_structure(sub_email) for sub_email in payload]))\n",
    "  else:\n",
    "    return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "c20eVd4aB7Oe"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_conter(emails):\n",
    "  structures = Counter()\n",
    "  for email in emails:\n",
    "    structure = get_email_structure(email)\n",
    "    structures[structure] += 1\n",
    "  return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGYrgwDeC14E",
    "outputId": "856cd1a2-0634-4676-a361-0a5a12d12a97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain,application/pgp-signature)', 66),\n",
       " ('multipart(text/plain,text/html)', 8),\n",
       " ('multipart(text/plain,text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain,application/octet-stream)', 2),\n",
       " ('multipart(text/plain,text/enriched)', 1),\n",
       " ('multipart(text/plain,application/ms-tnef,text/plain)', 1),\n",
       " ('multipart(multipart(text/plain,text/plain,text/plain),application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain,video/mng)', 1),\n",
       " ('multipart(text/plain,multipart(text/plain))', 1),\n",
       " ('multipart(text/plain,application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain,multipart(text/plain,text/plain),text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain,multipart(text/plain,text/plain),multipart(multipart(text/plain,application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain,application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_conter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRT8VzMIDTTa",
    "outputId": "d69cec08-80f3-4149-8634-52a43bfddd2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain,text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain,image/jpeg)', 3),\n",
       " ('multipart(text/html,application/octet-stream)', 2),\n",
       " ('multipart(text/plain,application/octet-stream)', 1),\n",
       " ('multipart(text/html,text/plain)', 1),\n",
       " ('multipart(multipart(text/html),application/octet-stream,image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain,text/html),image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_conter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ZW-jzN3QD1yo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8h98xLpEaYA"
   },
   "source": [
    "It seems that the ham emails are more often plain text, while spam has quite a lot of HTML. Moreover, quite a few ham emails are signed using PGP, while no spam is. In short, it seems that the email structure is useful information to have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9F2FrRyEbHp",
    "outputId": "912af312-fb96-45fd-9e30-a8054c34b8a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <12a1mailbot1@web.de>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32\tfor <zzzz@localhost>; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\n",
      "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\n",
      "Received : from dd_it7 ([210.97.77.167])\tby webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 13:09:41 +0100\n",
      "From : 12a1mailbot1@web.de\n",
      "Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);\t Sat, 24 Aug 2002 09:42:10 +0900\n",
      "To : dcek1a1@netsgo.com\n",
      "Subject : Life Insurance - Why Pay More?\n",
      "Date : Wed, 21 Aug 2002 20:31:57 -1600\n",
      "MIME-Version : 1.0\n",
      "Message-ID : <0103c1042001882DD_IT7@dd_it7>\n",
      "Content-Type : text/html; charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding : quoted-printable\n"
     ]
    }
   ],
   "source": [
    "for header, value in spam_emails[0].items():\n",
    "  print(header,\":\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HZvJiGTcEjS3",
    "outputId": "79d5a626-b455-4e99-c6bc-cd9048200ed5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Life Insurance - Why Pay More?'"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[0][\"Subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "OVctMMXPEn1r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "BuBA2O5vzeho"
   },
   "outputs": [],
   "source": [
    "X = np.array(ham_emails+spam_emails)\n",
    "y = np.array([0]*len(ham_emails)+[1]*len(spam_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Xj-nRxsT0iwx"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7cGBojV0spL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d86XJrG31Cro"
   },
   "source": [
    "Converting HTML to plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ITw7xUTW1DUJ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ZgyXkFHp1qVb"
   },
   "outputs": [],
   "source": [
    "def html_to_plain_text(html):\n",
    "  text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "  text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "  text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "  text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "  return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKpPWYJ-144r",
    "outputId": "c8ab9d02-d479-4dd7-f260-55756af5ccfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML><HEAD><TITLE></TITLE><META http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\"><STYLE>A:link {TEX-DECORATION: none}A:active {TEXT-DECORATION: none}A:visited {TEXT-DECORATION: none}A:hover {COLOR: #0033ff; TEXT-DECORATION: underline}</STYLE><META content=\"MSHTML 6.00.2713.1100\" name=\"GENERATOR\"></HEAD>\n",
      "<BODY text=\"#000000\" vLink=\"#0033ff\" link=\"#0033ff\" bgColor=\"#CCCC99\"><TABLE borderColor=\"#660000\" cellSpacing=\"0\" cellPadding=\"0\" border=\"0\" width=\"100%\"><TR><TD bgColor=\"#CCCC99\" valign=\"top\" colspan=\"2\" height=\"27\">\n",
      "<font size=\"6\" face=\"Arial, Helvetica, sans-serif\" color=\"#660000\">\n",
      "<b>OTC</b></font></TD></TR><TR><TD height=\"2\" bgcolor=\"#6a694f\">\n",
      "<font size=\"5\" face=\"Times New Roman, Times, serif\" color=\"#FFFFFF\">\n",
      "<b>&nbsp;Newsletter</b></font></TD><TD height=\"2\" bgcolor=\"#6a694f\"><div align=\"right\"><font color=\"#FFFFFF\">\n",
      "<b>Discover Tomorrow's Winners&nbsp;</b></font></div></TD></TR><TR><TD height=\"25\" colspan=\"2\" bgcolor=\"#CCCC99\"><table width=\"100%\" border=\"0\"  ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHKdXe6j18ak",
    "outputId": "921d5c7a-f84a-44ea-9cd6-febc014e0621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTC\n",
      " Newsletter\n",
      "Discover Tomorrow's Winners \n",
      "For Immediate Release\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Watch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\n",
      "Put CBYI on your watch list, acquire a position TODAY.\n",
      "REASONS TO INVEST IN CBYI\n",
      "A profitable company and is on track to beat ALL earnings estimates!\n",
      "One of the FASTEST growing distributors in environmental & safety equipment instruments.\n",
      "Excellent management team, several EXCLUSIVE contracts.  IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy & Environmental Research.\n",
      "RAPIDLY GROWING INDUSTRY\n",
      "Industry revenues exceed $900 million, estimates indicate that there could be as much as $25 billi ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GwQY00L2Bt0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii7RDMmI2IHL"
   },
   "source": [
    "Function to take email as input and return its content as plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Ke0c-YIz2ROC"
   },
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2iyi0WI2_wL",
    "outputId": "8724dd69-ebe3-461b-96ca-d16225c1e53f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OTC\n",
      " Newsletter\n",
      "Discover Tomorrow's Winners \n",
      "For Immediate Release\n",
      "Cal-Bay (Stock Symbol: CBYI)\n",
      "Wat ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "6SzQV5673Iu8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "EaFhP1Ww3YHU"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKrNj2225ha2",
    "outputId": "e74d9877-d0d8-4c09-8c1b-955d056fdd57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: replacing URLs requires the urlextract module.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import urlextract\n",
    "    url_extractor = urlextract.URLExtract()\n",
    "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
    "except ImportError:\n",
    "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
    "    url_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cROhg5Jk5Zck",
    "outputId": "8a9ad099-9a16-457b-fe3f-3bafe84772de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n",
       "       Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'to': 3, 'by': 3, 'jefferson': 2, 'i': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'ha': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'some': 1, 'interest': 1, 'quot': 1, 'http': 1, 'www': 1, 'postfun': 1, 'com': 1, 'pfp': 1, 'worboi': 1, 'html': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'thi': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1}),\n",
       "       Counter({'number': 5, 'http': 4, 'yahoo': 4, 's': 3, 'group': 3, 'com': 3, 'to': 3, 'in': 2, 'forteana': 2, 'martin': 2, 'an': 2, 'and': 2, 'memri': 2, 'we': 2, 'is': 2, 'unsubscrib': 2, 'y': 1, 'adamson': 1, 'wrote': 1, 'for': 1, 'altern': 1, 'rather': 1, 'more': 1, 'factual': 1, 'base': 1, 'rundown': 1, 'on': 1, 'hamza': 1, 'career': 1, 'includ': 1, 'hi': 1, 'belief': 1, 'that': 1, 'all': 1, 'non': 1, 'muslim': 1, 'yemen': 1, 'should': 1, 'be': 1, 'murder': 1, 'outright': 1, 'org': 1, 'bin': 1, 'articl': 1, 'cgi': 1, 'page': 1, 'archiv': 1, 'area': 1, 'ia': 1, 'id': 1, 'ianumb': 1, 'know': 1, 'how': 1, 'unbias': 1, 'don': 1, 't': 1, 'www': 1, 'guardian': 1, 'co': 1, 'uk': 1, 'elsewher': 1, 'journalist': 1, 'stori': 1, 'html': 1, 'rob': 1, 'sponsor': 1, 'dvd': 1, 'free': 1, 'p': 1, 'join': 1, 'now': 1, 'us': 1, 'click': 1, 'ptnumberybb': 1, 'nxieaa': 1, 'mvfiaa': 1, 'numbergsolb': 1, 'tm': 1, 'from': 1, 'thi': 1, 'send': 1, 'email': 1, 'egroup': 1, 'your': 1, 'use': 1, 'of': 1, 'subject': 1, 'doc': 1, 'info': 1, 'term': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "zN4LWpKQ5gNM"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_dLcQ2Q5dLv",
    "outputId": "1f47d332-e1f0-4307-9450-654091a82a41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HQgm_la56N8",
    "outputId": "b57cecf3-8435-4912-bd4b-9c6566a5dfe9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [112,  11,   9,   8,   3,   1,   0,   1,   3,   0,   1],\n",
       "       [ 92,   0,   1,   2,   3,   4,   5,   3,   1,   4,   2]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrNe96UQ7HKw"
   },
   "source": [
    "112 means that there are 112 words that are not part of the vocabulary. The next 11 means the first word in the vocabulary is present 11 times and 9 means second words present 9 times and so on....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBcN5nm758xg",
    "outputId": "497c70c6-9d83-4a5d-8048-a025e5d42135"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': 8,\n",
       " 'and': 3,\n",
       " 'com': 7,\n",
       " 'http': 5,\n",
       " 'in': 10,\n",
       " 'number': 6,\n",
       " 'of': 2,\n",
       " 'the': 1,\n",
       " 'to': 4,\n",
       " 'yahoo': 9}"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "vS2M_e2U6FF8"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3gBTScM6LL7",
    "outputId": "78b4a8ad-9a18-4924-b56f-ef9031d54950"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.983, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.984, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.993, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9862500000000001"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAfNkxls6PxP",
    "outputId": "8aacda65-b8c1-4287-cfc7-3355af74bc4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 96.88%\n",
      "Recall: 97.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pl4kJOV66-2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Email Classifer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
